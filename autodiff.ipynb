{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=32, size=56Bytes) Value(data=2, size=56Bytes)\n",
      "Value(data=34, size=56Bytes)\n",
      "Value(data=64, size=56Bytes)\n",
      "Value(data=96, size=56Bytes)\n",
      "1.0 3.0 32.0\n"
     ]
    }
   ],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='') -> None:\n",
    "        self.value: int = data\n",
    "        self.grad = .0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value(data={self.value}, size={sys.getsizeof(self)}Bytes)\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # to make a+1.0 work\n",
    "        out = Value(self.value + other.value, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.value * other.value, (self, other), \"*\")\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return other * self\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.value\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), \"tanh\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = (1 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.value\n",
    "        out = Value(math.exp(x), (self,))\n",
    "        def _backward():\n",
    "            self.grad += out.value * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * other**(-1)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supports int/float powers\"\n",
    "        out = Value(self.value**other, (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * self.value**(other-1) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def backward(self):\n",
    "        # build the topological graph to order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "a = Value(32)\n",
    "b = Value(2)\n",
    "print(a, b)\n",
    "print(a+b)\n",
    "print(a*b)\n",
    "d = a*b + a\n",
    "print(d)\n",
    "\n",
    "d.backward()\n",
    "\n",
    "print(d.grad, a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Value(data=64, size=56Bytes), Value(data=32, size=56Bytes)} +\n"
     ]
    }
   ],
   "source": [
    "print(d._prev, d._op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 5.0\n"
     ]
    }
   ],
   "source": [
    "# Bug, should be 2 instead of 1, in the function _backward of addition,\n",
    "#  we are overriding the gradient value (1), when self is the same object as other\n",
    "#  when we use a variable more then ones\n",
    "#  we should accumulate those gradients, multivariate case of chain rule\n",
    "s = a + a\n",
    "s.backward()\n",
    "print(s.grad, a.grad)\n",
    "# Expected 2, got 1 for s.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.0\n"
     ]
    }
   ],
   "source": [
    "# Bug fixed\n",
    "a = Value(32)\n",
    "s = a + a\n",
    "s.backward()\n",
    "print(s.grad, a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run back propagation, we need to store all the nodes in a topological order, then run the function in reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.5, size=56Bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Value(data=-2.0, size=56Bytes)"
      ]
     },
     "execution_count": 973,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(4.0)\n",
    "print(a/b)\n",
    "a-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From scalars, gradients, neurons, and backpropagation To a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9999834782734754, size=56Bytes)"
      ]
     },
     "execution_count": 974,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, n) -> None:\n",
    "        self.w = [Value(np.random.randn()) for _ in range(n)]\n",
    "        self.b = Value(np.random.randn())\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        a = sum((wi+xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        return a.tanh()\n",
    "    \n",
    "    \n",
    "x = [2.0, 3.0]\n",
    "n = Neuron(2)\n",
    "n(x) # python will call n.__call__(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9998321227272446, size=56Bytes),\n",
       " Value(data=0.9999970892341399, size=56Bytes),\n",
       " Value(data=0.9999997132102807, size=56Bytes)]"
      ]
     },
     "execution_count": 975,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer:\n",
    "    def __init__(self, nin, nout) -> None:\n",
    "        self.nin = nin\n",
    "        self.nout = nout\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for neuron in self.neurons:\n",
    "            params.extend(neuron.parameters())\n",
    "        return params\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return [neuron(x) for neuron in self.neurons]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)[0] if self.nout == 1 else self.forward(x)\n",
    "    \n",
    "l = Layer(2, 3)\n",
    "l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, nin, nouts) -> None:\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.parameters())\n",
    "        return params\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9717841984379162, size=56Bytes)"
      ]
     },
     "execution_count": 977,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = [2.0, 3.0]\n",
    "mlp = MLP(2, [3, 4, 1]) \n",
    "mlp(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9717841984379162, size=56Bytes),\n",
       " Value(data=0.907588246803216, size=56Bytes),\n",
       " Value(data=0.9717841984379162, size=56Bytes)]"
      ]
     },
     "execution_count": 978,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "xs = [\n",
    "    [2.0, 3.0, 1.0, 15.0],\n",
    "    [1.0, 2.0, 3.0, 4.0],\n",
    "    [3.0, 2.0, 1.0, 0.0]\n",
    "]\n",
    "target = [1.0, 0.0, 1.0]\n",
    "pred = [mlp(xs[i]) for i in range(3)]\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's define a loss to mesure the error of the model, we will use the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.8253086886509172, size=56Bytes)"
      ]
     },
     "execution_count": 979,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse(pred, target):\n",
    "    return sum((p-t)**2 for p, t in zip(pred, target))\n",
    "\n",
    "loss = mse(pred, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05309032212159961"
      ]
     },
     "execution_count": 981,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the gradients\n",
    "mlp.layers[0].neurons[1].w[1].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-2.012293840208911, size=56Bytes),\n",
       " Value(data=0.03806197044696347, size=56Bytes),\n",
       " Value(data=-0.451750955485669, size=56Bytes),\n",
       " Value(data=-0.5000302410899579, size=56Bytes),\n",
       " Value(data=-0.07118289509848812, size=56Bytes),\n",
       " Value(data=-0.7749125144913039, size=56Bytes),\n",
       " Value(data=1.8486488501662062, size=56Bytes),\n",
       " Value(data=0.7882446436816126, size=56Bytes),\n",
       " Value(data=0.9763373923984746, size=56Bytes),\n",
       " Value(data=0.7060136155333891, size=56Bytes),\n",
       " Value(data=1.2299360902008314, size=56Bytes),\n",
       " Value(data=-1.1199945670693077, size=56Bytes),\n",
       " Value(data=0.8398545059033957, size=56Bytes),\n",
       " Value(data=-0.5706141864098181, size=56Bytes),\n",
       " Value(data=-1.6878816028244596, size=56Bytes),\n",
       " Value(data=-0.9573114894061031, size=56Bytes),\n",
       " Value(data=0.5846133861466069, size=56Bytes),\n",
       " Value(data=-1.7019930085764075, size=56Bytes),\n",
       " Value(data=1.2285268781185719, size=56Bytes),\n",
       " Value(data=0.34701101078249597, size=56Bytes),\n",
       " Value(data=1.7453417621342378, size=56Bytes),\n",
       " Value(data=-1.859379099392286, size=56Bytes),\n",
       " Value(data=1.236775702618236, size=56Bytes),\n",
       " Value(data=-0.2546737685192902, size=56Bytes),\n",
       " Value(data=-0.2508247994215154, size=56Bytes),\n",
       " Value(data=-1.710371461406278, size=56Bytes),\n",
       " Value(data=-0.5563021608530474, size=56Bytes),\n",
       " Value(data=-0.25236134662179227, size=56Bytes),\n",
       " Value(data=1.097358204482623, size=56Bytes),\n",
       " Value(data=0.25127315937065503, size=56Bytes)]"
      ]
     },
     "execution_count": 982,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights and biases of all the network\n",
    "mlp.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07118289509848812"
      ]
     },
     "execution_count": 983,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.layers[0].neurons[1].w[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0717137983197041"
      ]
     },
     "execution_count": 984,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.01\n",
    "for p in mlp.parameters():\n",
    "    p.value -= lr * p.grad\n",
    "\n",
    "# check the gradients\n",
    "mlp.layers[0].neurons[1].w[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.8135930674366098, size=56Bytes)"
      ]
     },
     "execution_count": 985,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, we expect the loss to decrease\n",
    "pred = [mlp(xs[i]) for i in range(3)]\n",
    "loss = mse(pred, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreased. Yaaay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 -> Loss: 0.8135930674366098\n",
      "Epoch: 1 -> Loss: 0.8004576068829902\n",
      "Epoch: 2 -> Loss: 0.7887047518081861\n",
      "Epoch: 3 -> Loss: 0.7757506672016046\n",
      "Epoch: 4 -> Loss: 0.7614428013181432\n",
      "Epoch: 5 -> Loss: 0.7456104823329142\n",
      "Epoch: 6 -> Loss: 0.7280649734208762\n",
      "Epoch: 7 -> Loss: 0.7086010993840819\n",
      "Epoch: 8 -> Loss: 0.687001405818821\n",
      "Epoch: 9 -> Loss: 0.6630441865526226\n",
      "Epoch: 10 -> Loss: 0.6365171205308876\n",
      "Epoch: 11 -> Loss: 0.6072385749142097\n",
      "Epoch: 12 -> Loss: 0.5750886002720758\n",
      "Epoch: 13 -> Loss: 0.540050814569879\n",
      "Epoch: 14 -> Loss: 0.5022640964078682\n",
      "Epoch: 15 -> Loss: 0.4620786263675388\n",
      "Epoch: 16 -> Loss: 0.42010422727073643\n",
      "Epoch: 17 -> Loss: 0.37723164480314153\n",
      "Epoch: 18 -> Loss: 0.3346034834795187\n",
      "Epoch: 19 -> Loss: 0.29351725694561065\n",
      "Epoch: 20 -> Loss: 0.25526305033628155\n",
      "Epoch: 21 -> Loss: 0.22092876114436624\n",
      "Epoch: 22 -> Loss: 0.1912307946546154\n",
      "Epoch: 23 -> Loss: 0.1664275013111327\n",
      "Epoch: 24 -> Loss: 0.14634078078009635\n",
      "Epoch: 25 -> Loss: 0.13046619852478147\n",
      "Epoch: 26 -> Loss: 0.11812195065319758\n",
      "Epoch: 27 -> Loss: 0.1085863915603351\n",
      "Epoch: 28 -> Loss: 0.10119452633356651\n",
      "Epoch: 29 -> Loss: 0.09538795013490949\n",
      "Epoch: 30 -> Loss: 0.09072802512735738\n",
      "Epoch: 31 -> Loss: 0.08688672924346216\n",
      "Epoch: 32 -> Loss: 0.08362754009757552"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 466.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 33 -> Loss: 0.08078440087020584\n",
      "Epoch: 34 -> Loss: 0.07824289959277722\n",
      "Epoch: 35 -> Loss: 0.07592515651328174\n",
      "Epoch: 36 -> Loss: 0.07377847640226096\n",
      "Epoch: 37 -> Loss: 0.07176720368291065\n",
      "Epoch: 38 -> Loss: 0.06986705885195496\n",
      "Epoch: 39 -> Loss: 0.06806128923313325\n",
      "Epoch: 40 -> Loss: 0.06633809606915808\n",
      "Epoch: 41 -> Loss: 0.06468893552430495\n",
      "Epoch: 42 -> Loss: 0.0631074065356759\n",
      "Epoch: 43 -> Loss: 0.06158852719869091\n",
      "Epoch: 44 -> Loss: 0.06012826573132915\n",
      "Epoch: 45 -> Loss: 0.05872323698837785\n",
      "Epoch: 46 -> Loss: 0.057370506051065334\n",
      "Epoch: 47 -> Loss: 0.05606746081618458\n",
      "Epoch: 48 -> Loss: 0.054811728946583545\n",
      "Epoch: 49 -> Loss: 0.0536011233110112\n",
      "Epoch: 50 -> Loss: 0.052433605719298844\n",
      "Epoch: 51 -> Loss: 0.05130726241780678\n",
      "Epoch: 52 -> Loss: 0.05022028715961525\n",
      "Epoch: 53 -> Loss: 0.04917096916919771\n",
      "Epoch: 54 -> Loss: 0.0481576842844845\n",
      "Epoch: 55 -> Loss: 0.047178888175291214\n",
      "Epoch: 56 -> Loss: 0.04623311093124581\n",
      "Epoch: 57 -> Loss: 0.04531895256477804\n",
      "Epoch: 58 -> Loss: 0.04443507913658751\n",
      "Epoch: 59 -> Loss: 0.04358021931495474\n",
      "Epoch: 60 -> Loss: 0.042753161247115534\n",
      "Epoch: 61 -> Loss: 0.04195274966397236\n",
      "Epoch: 62 -> Loss: 0.04117788316717034\n",
      "Epoch: 63 -> Loss: 0.04042751166543475\n",
      "Epoch: 64 -> Loss: 0.039700633938567\n",
      "Epoch: 65 -> Loss: 0.03899629531485445\n",
      "Epoch: 66 -> Loss: 0.03831358545233486\n",
      "Epoch: 67 -> Loss: 0.037651636217297335\n",
      "Epoch: 68 -> Loss: 0.03700961965522094\n",
      "Epoch: 69 -> Loss: 0.03638674605044927\n",
      "Epoch: 70 -> Loss: 0.035782262071529555\n",
      "Epoch: 71 -> Loss: 0.03519544899948952\n",
      "Epoch: 72 -> Loss: 0.03462562103649045\n",
      "Epoch: 73 -> Loss: 0.03407212369235382\n",
      "Epoch: 74 -> Loss: 0.033534332246464044\n",
      "Epoch: 75 -> Loss: 0.033011650282526964\n",
      "Epoch: 76 -> Loss: 0.032503508293629524\n",
      "Epoch: 77 -> Loss: 0.032009362355022226\n",
      "Epoch: 78 -> Loss: 0.03152869286202501\n",
      "Epoch: 79 -> Loss: 0.031061003330451933\n",
      "Epoch: 80 -> Loss: 0.030605819256962076\n",
      "Epoch: 81 -> Loss: 0.030162687036759853\n",
      "Epoch: 82 -> Loss: 0.029731172936109888\n",
      "Epoch: 83 -> Loss: 0.029310862117173132\n",
      "Epoch: 84 -> Loss: 0.02890135771272787\n",
      "Epoch: 85 -> Loss: 0.028502279948404194\n",
      "Epoch: 86 -> Loss: 0.028113265310129164\n",
      "Epoch: 87 -> Loss: 0.027733965754556448\n",
      "Epoch: 88 -> Loss: 0.027364047960332115\n",
      "Epoch: 89 -> Loss: 0.027003192618130275\n",
      "Epoch: 90 -> Loss: 0.02665109375747467\n",
      "Epoch: 91 -> Loss: 0.026307458108445207\n",
      "Epoch: 92 -> Loss: 0.025972004496453205\n",
      "Epoch: 93 -> Loss: 0.025644463268348662\n",
      "Epoch: 94 -> Loss: 0.02532457574820614\n",
      "Epoch: 95 -> Loss: 0.0250120937212144\n",
      "Epoch: 96 -> Loss: 0.024706778944171122\n",
      "Epoch: 97 -> Loss: 0.024408402681159732\n",
      "Epoch: 98 -> Loss: 0.024116745263057356\n",
      "Epoch: 99 -> Loss: 0.02383159566959359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Value(data=0.023552751132743188, size=56Bytes)"
      ]
     },
     "execution_count": 987,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 0.01\n",
    "for i in tqdm(range(epochs)):\n",
    "    pred = [mlp(xs[i]) for i in range(3)]\n",
    "    loss = mse(pred, target)\n",
    "    for p in mlp.parameters():\n",
    "        p.grad = .0\n",
    "\n",
    "    loss.backward()\n",
    "    for p in mlp.parameters():\n",
    "        p.value -= lr * p.grad\n",
    "    loss = mse(pred, target)\n",
    "\n",
    "    # lr\n",
    "    if i % 100 == 0:\n",
    "        lr *= 0.8\n",
    "\n",
    "    print(f\"Epoch: {i} -> Loss: {loss.value}\")\n",
    "\n",
    "pred = [mlp(xs[i]) for i in range(3)]\n",
    "loss = mse(pred, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wooooooow! It's working! Loss decreased by a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.8929492495800423, size=56Bytes),\n",
       " Value(data=0.02515998413733679, size=56Bytes),\n",
       " Value(data=0.8929492495800423, size=56Bytes)]"
      ]
     },
     "execution_count": 988,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not perfect, but it's a good start."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
