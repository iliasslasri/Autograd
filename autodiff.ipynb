{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=32, size=56Bytes) Value(data=2, size=56Bytes)\n",
      "Value(data=34, size=56Bytes)\n",
      "Value(data=64, size=56Bytes)\n",
      "Value(data=96, size=56Bytes)\n",
      "1.0 3.0 32.0\n"
     ]
    }
   ],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='') -> None:\n",
    "        self.value: int = data\n",
    "        self.grad = .0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value(data={self.value}, size={sys.getsizeof(self)}Bytes)\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # to make a+1.0 work\n",
    "        out = Value(self.value + other.value, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.value * other.value, (self, other), \"*\")\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return other * self\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.value\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), \"tanh\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = (1 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.value\n",
    "        out = Value(math.exp(x), (self,))\n",
    "        def _backward():\n",
    "            self.grad += out.value * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * other**(-1)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supports int/float powers\"\n",
    "        out = Value(self.value**other, (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * self.value**(other-1) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def backward(self):\n",
    "        # build the topological graph to order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "a = Value(32)\n",
    "b = Value(2)\n",
    "print(a, b)\n",
    "print(a+b)\n",
    "print(a*b)\n",
    "d = a*b + a\n",
    "print(d)\n",
    "\n",
    "d.backward()\n",
    "\n",
    "print(d.grad, a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Value(data=64, size=56Bytes), Value(data=32, size=56Bytes)} +\n"
     ]
    }
   ],
   "source": [
    "print(d._prev, d._op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 5.0\n"
     ]
    }
   ],
   "source": [
    "# Bug, should be 2 instead of 1, in the function _backward of addition,\n",
    "#  we are overriding the gradient value (1), when self is the same object as other\n",
    "#  when we use a variable more then ones\n",
    "#  we should accumulate those gradients, multivariate case of chain rule\n",
    "s = a + a\n",
    "s.backward()\n",
    "print(s.grad, a.grad)\n",
    "# Expected 2, got 1 for s.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.0\n"
     ]
    }
   ],
   "source": [
    "# Bug fixed\n",
    "a = Value(32)\n",
    "s = a + a\n",
    "s.backward()\n",
    "print(s.grad, a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run back propagation, we need to store all the nodes in a topological order, then run the function in reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.5, size=56Bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Value(data=-2.0, size=56Bytes)"
      ]
     },
     "execution_count": 1116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(4.0)\n",
    "print(a/b)\n",
    "a-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From scalars, gradients, neurons, and backpropagation To a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9999996303486614, size=56Bytes)"
      ]
     },
     "execution_count": 1117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, n) -> None:\n",
    "        self.w = [Value(np.random.randn()) for _ in range(n)]\n",
    "        self.b = Value(np.random.randn())\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        a = sum((wi+xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        return a.tanh()\n",
    "    \n",
    "    \n",
    "x = [2.0, 3.0]\n",
    "n = Neuron(2)\n",
    "n(x) # python will call n.__call__(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.994048406796414, size=56Bytes),\n",
       " Value(data=0.9999721590140617, size=56Bytes),\n",
       " Value(data=0.9992224787031223, size=56Bytes)]"
      ]
     },
     "execution_count": 1118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer:\n",
    "    def __init__(self, nin, nout) -> None:\n",
    "        self.nin = nin\n",
    "        self.nout = nout\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for neuron in self.neurons:\n",
    "            params.extend(neuron.parameters())\n",
    "        return params\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return [neuron(x) for neuron in self.neurons]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)[0] if self.nout == 1 else self.forward(x)\n",
    "    \n",
    "l = Layer(2, 3)\n",
    "l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, nin, nouts) -> None:\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.parameters())\n",
    "        return params\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9941342908150541, size=56Bytes)"
      ]
     },
     "execution_count": 1120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = [2.0, 3.0]\n",
    "mlp = MLP(2, [3, 4, 1]) \n",
    "mlp(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9941342908150541, size=56Bytes),\n",
       " Value(data=0.994054980073154, size=56Bytes),\n",
       " Value(data=0.9941342908150541, size=56Bytes)]"
      ]
     },
     "execution_count": 1121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "xs = [\n",
    "    [2.0, 3.0, 1.0, 15.0],\n",
    "    [1.0, 2.0, 3.0, 4.0],\n",
    "    [3.0, 2.0, 1.0, 0.0]\n",
    "]\n",
    "target = [1.0, 0.0, 1.0]\n",
    "pred = [mlp(xs[i]) for i in range(3)]\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's define a loss to mesure the error of the model, we will use the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9882141164967232, size=56Bytes)"
      ]
     },
     "execution_count": 1122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse(pred, target):\n",
    "    return sum((p-t)**2 for p, t in zip(pred, target))\n",
    "\n",
    "loss = mse(pred, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1123,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.669147383293113e-06"
      ]
     },
     "execution_count": 1124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the gradients\n",
    "mlp.layers[0].neurons[1].w[1].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.6529901412410457, size=56Bytes),\n",
       " Value(data=0.5297575647074153, size=56Bytes),\n",
       " Value(data=-0.5004728551461568, size=56Bytes),\n",
       " Value(data=-0.5075937218740927, size=56Bytes),\n",
       " Value(data=1.3501966815674855, size=56Bytes),\n",
       " Value(data=-1.1634127460098471, size=56Bytes),\n",
       " Value(data=0.7794399527689354, size=56Bytes),\n",
       " Value(data=-1.9021398592335295, size=56Bytes),\n",
       " Value(data=-1.9863336468039758, size=56Bytes),\n",
       " Value(data=1.422881878722518, size=56Bytes),\n",
       " Value(data=0.35895657270957204, size=56Bytes),\n",
       " Value(data=-0.7954659800749525, size=56Bytes),\n",
       " Value(data=0.9306981349597084, size=56Bytes),\n",
       " Value(data=0.6853410324777836, size=56Bytes),\n",
       " Value(data=0.645352813977939, size=56Bytes),\n",
       " Value(data=0.4229889886454633, size=56Bytes),\n",
       " Value(data=0.7830763129461916, size=56Bytes),\n",
       " Value(data=-0.6303842913269332, size=56Bytes),\n",
       " Value(data=-0.2760073839987229, size=56Bytes),\n",
       " Value(data=0.7735478094197789, size=56Bytes),\n",
       " Value(data=1.6995433778260285, size=56Bytes),\n",
       " Value(data=-1.038543181417918, size=56Bytes),\n",
       " Value(data=1.7224015551066707, size=56Bytes),\n",
       " Value(data=1.004021087766309, size=56Bytes),\n",
       " Value(data=-0.46713886630147194, size=56Bytes),\n",
       " Value(data=-1.5514603755506353, size=56Bytes),\n",
       " Value(data=-0.027757677450172973, size=56Bytes),\n",
       " Value(data=-0.25197056346853186, size=56Bytes),\n",
       " Value(data=0.5503367065183901, size=56Bytes),\n",
       " Value(data=0.19613243122729762, size=56Bytes)]"
      ]
     },
     "execution_count": 1125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights and biases of all the network\n",
    "mlp.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3501966815674855"
      ]
     },
     "execution_count": 1126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.layers[0].neurons[1].w[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3501966148760116"
      ]
     },
     "execution_count": 1127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.01\n",
    "for p in mlp.parameters():\n",
    "    p.value -= lr * p.grad\n",
    "\n",
    "# check the gradients\n",
    "mlp.layers[0].neurons[1].w[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.988186949329495, size=56Bytes)"
      ]
     },
     "execution_count": 1128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, we expect the loss to decrease\n",
    "pred = [mlp(xs[i]) for i in range(3)]\n",
    "loss = mse(pred, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreased. Yaaay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 -> Loss: 0.988186949329495\n",
      "Epoch: 1 -> Loss: 0.9881596582246531\n",
      "Epoch: 2 -> Loss: 0.9881322423396407\n",
      "Epoch: 3 -> Loss: 0.9881047008242994\n",
      "Epoch: 4 -> Loss: 0.9880770328207834\n",
      "Epoch: 5 -> Loss: 0.9880492374634737\n",
      "Epoch: 6 -> Loss: 0.9880213138788891\n",
      "Epoch: 7 -> Loss: 0.9879932611855988\n",
      "Epoch: 8 -> Loss: 0.9879650784941312\n",
      "Epoch: 9 -> Loss: 0.9879367649068829\n",
      "Epoch: 10 -> Loss: 0.9879083195180259\n",
      "Epoch: 11 -> Loss: 0.987879741413414\n",
      "Epoch: 12 -> Loss: 0.9878510296704881\n",
      "Epoch: 13 -> Loss: 0.9878221833581782\n",
      "Epoch: 14 -> Loss: 0.9877932015368078\n",
      "Epoch: 15 -> Loss: 0.9877640832579927\n",
      "Epoch: 16 -> Loss: 0.9877348275645422\n",
      "Epoch: 17 -> Loss: 0.9877054334903559\n",
      "Epoch: 18 -> Loss: 0.9876759000603218\n",
      "Epoch: 19 -> Loss: 0.9876462262902106\n",
      "Epoch: 20 -> Loss: 0.9876164111865694\n",
      "Epoch: 21 -> Loss: 0.9875864537466148\n",
      "Epoch: 22 -> Loss: 0.9875563529581237\n",
      "Epoch: 23 -> Loss: 0.9875261077993214\n",
      "Epoch: 24 -> Loss: 0.9874957172387722\n",
      "Epoch: 25 -> Loss: 0.9874651802352619\n",
      "Epoch: 26 -> Loss: 0.9874344957376849\n",
      "Epoch: 27 -> Loss: 0.9874036626849272\n",
      "Epoch: 28 -> Loss: 0.9873726800057461\n",
      "Epoch: 29 -> Loss: 0.9873415466186515\n",
      "Epoch: 30 -> Loss: 0.9873102614317822\n",
      "Epoch: 31 -> Loss: 0.9872788233427843\n",
      "Epoch: 32 -> Loss: 0.9872472312386835\n",
      "Epoch: 33 -> Loss: 0.9872154839957583\n",
      "Epoch: 34 -> Loss: 0.9871835804794117\n",
      "Epoch: 35 -> Loss: 0.9871515195440388\n",
      "Epoch: 36 -> Loss: 0.9871193000328953\n",
      "Epoch: 37 -> Loss: 0.9870869207779611\n",
      "Epoch: 38 -> Loss: 0.987054380599804\n",
      "Epoch: 39 -> Loss: 0.9870216783074417\n",
      "Epoch: 40 -> Loss: 0.9869888126982003\n",
      "Epoch: 41 -> Loss: 0.9869557825575707\n",
      "Epoch: 42 -> Loss: 0.986922586659065\n",
      "Epoch: 43 -> Loss: 0.9868892237640686\n",
      "Epoch: 44 -> Loss: 0.9868556926216903\n",
      "Epoch: 45 -> Loss: 0.9868219919686134\n",
      "Epoch: 46 -> Loss: 0.9867881205289379\n",
      "Epoch: 47 -> Loss: 0.9867540770140285\n",
      "Epoch: 48 -> Loss: 0.9867198601223538\n",
      "Epoch: 49 -> Loss: 0.9866854685393271\n",
      "Epoch: 50 -> Loss: 0.9866509009371419\n",
      "Epoch: 51 -> Loss: 0.986616155974608\n",
      "Epoch: 52 -> Loss: 0.9865812322969824\n",
      "Epoch: 53 -> Loss: 0.9865461285357995\n",
      "Epoch: 54 -> Loss: 0.9865108433086976\n",
      "Epoch: 55 -> Loss: 0.9864753752192438\n",
      "Epoch: 56 -> Loss: 0.9864397228567552\n",
      "Epoch: 57 -> Loss: 0.986403884796118\n",
      "Epoch: 58 -> Loss: 0.986367859597604\n",
      "Epoch: 59 -> Loss: 0.9863316458066835\n",
      "Epoch: 60 -> Loss: 0.9862952419538374\n",
      "Epoch: 61 -> Loss: 0.9862586465543622\n",
      "Epoch: 62 -> Loss: 0.986221858108178\n",
      "Epoch: 63 -> Loss: 0.9861848750996272\n",
      "Epoch: 64 -> Loss: 0.9861476959972757\n",
      "Epoch: 65 -> Loss: 0.986110319253707\n",
      "Epoch: 66 -> Loss: 0.9860727433053149\n",
      "Epoch: 67 -> Loss: 0.9860349665720928\n",
      "Epoch: 68 -> Loss: 0.9859969874574205\n",
      "Epoch: 69 -> Loss: 0.9859588043478449\n",
      "Epoch: 70 -> Loss: 0.985920415612861\n",
      "Epoch: 71 -> Loss: 0.9858818196046857\n",
      "Epoch: 72 -> Loss: 0.9858430146580328\n",
      "Epoch: 73 -> Loss: 0.9858039990898786\n",
      "Epoch: 74 -> Loss: 0.9857647711992285\n",
      "Epoch: 75 -> Loss: 0.9857253292668776\n",
      "Epoch: 76 -> Loss: 0.9856856715551696\n",
      "Epoch: 77 -> Loss: 0.9856457963077472\n",
      "Epoch: 78 -> Loss: 0.9856057017493052\n",
      "Epoch: 79 -> Loss: 0.9855653860853336\n",
      "Epoch: 80 -> Loss: 0.9855248475018604\n",
      "Epoch: 81 -> Loss: 0.985484084165189\n",
      "Epoch: 82 -> Loss: 0.9854430942216307\n",
      "Epoch: 83 -> Loss: 0.9854018757972335\n",
      "Epoch: 84 -> Loss: 0.9853604269975071\n",
      "Epoch: 85 -> Loss: 0.9853187459071429\n",
      "Epoch: 86 -> Loss: 0.9852768305897278\n",
      "Epoch: 87 -> Loss: 0.9852346790874579\n",
      "Epoch: 88 -> Loss: 0.9851922894208411\n",
      "Epoch: 89 -> Loss: 0.9851496595884007\n",
      "Epoch: 90 -> Loss: 0.985106787566371\n",
      "Epoch: 91 -> Loss: 0.9850636713083876\n",
      "Epoch: 92 -> Loss: 0.9850203087451743\n",
      "Epoch: 93 -> Loss: 0.9849766977842249\n",
      "Epoch: 94 -> Loss: 0.9849328363094767\n",
      "Epoch: 95 -> Loss: 0.9848887221809817\n",
      "Epoch: 96 -> Loss: 0.9848443532345722\n",
      "Epoch: 97 -> Loss: 0.9847997272815181\n",
      "Epoch: 98 -> Loss: 0.9847548421081811\n",
      "Epoch: 99 -> Loss: 0.9847096954756618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1924.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Value(data=0.9846642851194423, size=56Bytes)"
      ]
     },
     "execution_count": 1130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 0.01\n",
    "for i in tqdm(range(epochs)):\n",
    "    pred = [mlp(xs[i]) for i in range(3)]\n",
    "    loss = mse(pred, target)\n",
    "    for p in mlp.parameters():\n",
    "        p.grad = .0\n",
    "\n",
    "    loss.backward()\n",
    "    for p in mlp.parameters():\n",
    "        p.value -= lr * p.grad\n",
    "    loss = mse(pred, target)\n",
    "\n",
    "    # lr\n",
    "    # if i % 100 == 0:\n",
    "    #     lr *= 0.9 # decay the learning rate\n",
    "    # this is a simple problem no need to decay lr\n",
    "\n",
    "    print(f\"Epoch: {i} -> Loss: {loss.value}\")\n",
    "\n",
    "pred = [mlp(xs[i]) for i in range(3)]\n",
    "loss = mse(pred, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wooooooow! It's working! Loss decreased by a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9923472916551775, size=56Bytes),\n",
       " Value(data=0.9922434969448882, size=56Bytes),\n",
       " Value(data=0.9923472916551775, size=56Bytes)]"
      ]
     },
     "execution_count": 1131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not perfect, but it's a good start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget zero grad after each epoch, because the gradients are accumulated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
