{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=32 Value(data=2\n",
      "Value(data=34\n",
      "Value(data=64\n",
      "Value(data=96\n",
      "1.0 3.0 32.0\n"
     ]
    }
   ],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='') -> None:\n",
    "        self.value: int = data\n",
    "        self.grad = .0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value(data={self.value}\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # to make a+1.0 work\n",
    "        out = Value(self.value + other.value, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.value * other.value, (self, other), \"*\")\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return other * self\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.value\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), \"tanh\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = (1 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.value\n",
    "        out = Value(math.exp(x), (self,))\n",
    "        def _backward():\n",
    "            self.grad += out.value * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * other**(-1)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supports int/float powers\"\n",
    "        out = Value(self.value**other, (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * self.value**(other-1) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def backward(self):\n",
    "        # build the topological graph to order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "a = Value(32)\n",
    "b = Value(2)\n",
    "print(a, b)\n",
    "print(a+b)\n",
    "print(a*b)\n",
    "d = a*b + a\n",
    "print(d)\n",
    "\n",
    "d.backward()\n",
    "\n",
    "print(d.grad, a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Value(data=64, Value(data=32} +\n"
     ]
    }
   ],
   "source": [
    "print(d._prev, d._op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 5.0\n"
     ]
    }
   ],
   "source": [
    "# Bug, should be 2 instead of 1, in the function _backward of addition,\n",
    "#  we are overriding the gradient value (1), when self is the same object as other\n",
    "#  when we use a variable more then ones\n",
    "#  we should accumulate those gradients, multivariate case of chain rule\n",
    "s = a + a\n",
    "s.backward()\n",
    "print(s.grad, a.grad)\n",
    "# Expected 2, got 1 for s.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.0\n"
     ]
    }
   ],
   "source": [
    "# Bug fixed\n",
    "a = Value(32)\n",
    "s = a + a\n",
    "s.backward()\n",
    "print(s.grad, a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run back propagation, we need to store all the nodes in a topological order, then run the function in reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Value(data=-2.0"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(4.0)\n",
    "print(a/b)\n",
    "a-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From scalars, gradients, neurons, and backpropagation To a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9999989510163969"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, n) -> None:\n",
    "        self.w = [Value(np.random.randn()) for _ in range(n)]\n",
    "        self.b = Value(np.random.randn())\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        a = sum((wi+xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        return a.tanh()\n",
    "    \n",
    "    \n",
    "x = [2.0, 3.0]\n",
    "n = Neuron(2)\n",
    "n(x) # python will call n.__call__(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9999925739334281,\n",
       " Value(data=0.999973652567766,\n",
       " Value(data=0.9999955858429052]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer:\n",
    "    def __init__(self, nin, nout) -> None:\n",
    "        self.nin = nin\n",
    "        self.nout = nout\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for neuron in self.neurons:\n",
    "            params.extend(neuron.parameters())\n",
    "        return params\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return [neuron(x) for neuron in self.neurons]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)[0] if self.nout == 1 else self.forward(x)\n",
    "    \n",
    "l = Layer(2, 3)\n",
    "l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, nin, nouts) -> None:\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.parameters())\n",
    "        return params\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9948751482727038"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = [2.0, 3.0]\n",
    "mlp = MLP(2, [3, 4, 1]) \n",
    "mlp(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9948751482727038,\n",
       " Value(data=0.9255864190976271,\n",
       " Value(data=0.9948751482727038]"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "xs = [\n",
    "    [2.0, 3.0, 1.0, 15.0],\n",
    "    [1.0, 2.0, 3.0, 4.0],\n",
    "    [3.0, 2.0, 1.0, 0.0]\n",
    "]\n",
    "target = [1.0, 0.0, 1.0]\n",
    "pred = [mlp(xs[i]) for i in range(3)]\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's define a loss to mesure the error of the model, we will use the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.8567627474284216"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse(pred, target):\n",
    "    return sum((p-t)**2 for p, t in zip(pred, target))\n",
    "\n",
    "loss = mse(pred, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003600700850319558"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the gradients\n",
    "mlp.layers[0].neurons[1].w[1].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-1.1745984990765248,\n",
       " Value(data=-1.9079705594284473,\n",
       " Value(data=-1.063660778987389,\n",
       " Value(data=0.11912046977656406,\n",
       " Value(data=-1.6816276151023937,\n",
       " Value(data=1.3902314898828079,\n",
       " Value(data=-1.9071027640431564,\n",
       " Value(data=-0.1895946584929257,\n",
       " Value(data=-1.5856813359838235,\n",
       " Value(data=0.19827130943298815,\n",
       " Value(data=-0.32833234581224435,\n",
       " Value(data=0.7273317367482757,\n",
       " Value(data=-0.4288962787096917,\n",
       " Value(data=0.09582058896184041,\n",
       " Value(data=-0.7471910167898416,\n",
       " Value(data=-1.3686457254791715,\n",
       " Value(data=-2.020450802650164,\n",
       " Value(data=0.21318803384403726,\n",
       " Value(data=0.9364294046023246,\n",
       " Value(data=0.6412825808785294,\n",
       " Value(data=0.979869139612908,\n",
       " Value(data=-0.8162891143232834,\n",
       " Value(data=-1.75637302588548,\n",
       " Value(data=-0.8161966498560191,\n",
       " Value(data=-2.1703793279886114,\n",
       " Value(data=2.085807633793719,\n",
       " Value(data=0.11655715952430847,\n",
       " Value(data=-0.3431429078876648,\n",
       " Value(data=0.49221191638528705,\n",
       " Value(data=0.5360250534255223]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights and biases of all the network\n",
    "mlp.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.6816276151023937"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.layers[0].neurons[1].w[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.681663622110897"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.01\n",
    "for p in mlp.parameters():\n",
    "    p.value -= lr * p.grad\n",
    "\n",
    "# check the gradients\n",
    "mlp.layers[0].neurons[1].w[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.8495386963346085"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, we expect the loss to decrease\n",
    "pred = [mlp(xs[i]) for i in range(3)]\n",
    "loss = mse(pred, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreased. Yaaay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 -> Loss: 0.8495386963346085\n",
      "Epoch: 1 -> Loss: 0.8416987366473648\n",
      "Epoch: 2 -> Loss: 0.8331752616815645\n",
      "Epoch: 3 -> Loss: 0.8238924426108042\n",
      "Epoch: 4 -> Loss: 0.8137653515488731\n",
      "Epoch: 5 -> Loss: 0.802699091415806\n",
      "Epoch: 6 -> Loss: 0.7905879938301942\n",
      "Epoch: 7 -> Loss: 0.7773149798241458\n",
      "Epoch: 8 -> Loss: 0.762751225081122\n",
      "Epoch: 9 -> Loss: 0.7467563352528163\n",
      "Epoch: 10 -> Loss: 0.7291793210071186\n",
      "Epoch: 11 -> Loss: 0.7098607683136523\n",
      "Epoch: 12 -> Loss: 0.6886367240401775\n",
      "Epoch: 13 -> Loss: 0.6653449477552339\n",
      "Epoch: 14 -> Loss: 0.6398342882837845\n",
      "Epoch: 15 -> Loss: 0.6119779714141635\n",
      "Epoch: 16 -> Loss: 0.5816914386411618\n",
      "Epoch: 17 -> Loss: 0.5489549190971685\n",
      "Epoch: 18 -> Loss: 0.5138399855054666\n",
      "Epoch: 19 -> Loss: 0.4765378136824522\n",
      "Epoch: 20 -> Loss: 0.43738477325846337\n",
      "Epoch: 21 -> Loss: 0.3968787229565336\n",
      "Epoch: 22 -> Loss: 0.35567789153038315\n",
      "Epoch: 23 -> Loss: 0.3145748777526356\n",
      "Epoch: 24 -> Loss: 0.2744423718862616\n",
      "Epoch: 25 -> Loss: 0.2361547589966109\n",
      "Epoch: 26 -> Loss: 0.2004986780202729\n",
      "Epoch: 27 -> Loss: 0.1680917851812384\n",
      "Epoch: 28 -> Loss: 0.1393285160703522\n",
      "Epoch: 29 -> Loss: 0.11436380777095666\n",
      "Epoch: 30 -> Loss: 0.09313402347385183\n",
      "Epoch: 31 -> Loss: 0.07540429661205472\n",
      "Epoch: 32 -> Loss: 0.060827146328630344\n",
      "Epoch: 33 -> Loss: 0.048998806797370315\n",
      "Epoch: 34 -> Loss: 0.039504740199631364\n",
      "Epoch: 35 -> Loss: 0.03195122829431771\n",
      "Epoch: 36 -> Loss: 0.02598386557616517\n",
      "Epoch: 37 -> Loss: 0.02129574986856207\n",
      "Epoch: 38 -> Loss: 0.017628611655533786\n",
      "Epoch: 39 -> Loss: 0.01476971314259279\n",
      "Epoch: 40 -> Loss: 0.012546627331850963\n",
      "Epoch: 41 -> Loss: 0.010821295831078643\n",
      "Epoch: 42 -> Loss: 0.009484196298863986\n",
      "Epoch: 43 -> Loss: 0.008449051190748695\n",
      "Epoch: 44 -> Loss: 0.007648254325769665\n",
      "Epoch: 45 -> Loss: 0.007029042636389059\n",
      "Epoch: 46 -> Loss: 0.006550362086140234\n",
      "Epoch: 47 -> Loss: 0.0061803421671197505\n",
      "Epoch: 48 -> Loss: 0.005894284028437618\n",
      "Epoch: 49 -> Loss: 0.005673071220365489\n",
      "Epoch: 50 -> Loss: 0.005501922078092865\n",
      "Epoch: 51 -> Loss: 0.005369414829004545\n",
      "Epoch: 52 -> Loss: 0.005266728431526802\n",
      "Epoch: 53 -> Loss: 0.005187052925897807\n",
      "Epoch: 54 -> Loss: 0.005125132323699784\n",
      "Epoch: 55 -> Loss: 0.005076910748895887\n",
      "Epoch: 56 -> Loss: 0.0050392587964708715\n",
      "Epoch: 57 -> Loss: 0.005009762087658857\n",
      "Epoch: 58 -> Loss: 0.004986557977122975\n",
      "Epoch: 59 -> Loss: 0.004968209497734193\n",
      "Epoch: 60 -> Loss: 0.004953608079173147\n",
      "Epoch: 61 -> Loss: 0.004941898487218377\n",
      "Epoch: 62 -> Loss: 0.004932420915772992\n",
      "Epoch: 63 -> Loss: 0.00492466631558725\n",
      "Epoch: 64 -> Loss: 0.004918241935581706\n",
      "Epoch: 65 -> Loss: 0.004912844742496128\n",
      "Epoch: 66 -> Loss: 0.004908240917615603\n",
      "Epoch: 67 -> Loss: 0.004904250040933535\n",
      "Epoch: 68 -> Loss: 0.004900732890812101\n",
      "Epoch: 69 -> Loss: 0.004897582032340824\n",
      "Epoch: 70 -> Loss: 0.004894714556703244\n",
      "Epoch: 71 -> Loss: 0.0048920664797258585\n",
      "Epoch: 72 -> Loss: 0.00488958842028222\n",
      "Epoch: 73 -> Loss: 0.004887242265985778\n",
      "Epoch: 74 -> Loss: 0.004884998600514671\n",
      "Epoch: 75 -> Loss: 0.0048828347185141795\n",
      "Epoch: 76 -> Loss: 0.004880733093817634\n",
      "Epoch: 77 -> Loss: 0.004878680197420259\n",
      "Epoch: 78 -> Loss: 0.004876665585312271\n",
      "Epoch: 79 -> Loss: 0.004874681194536331\n",
      "Epoch: 80 -> Loss: 0.004872720799918517\n",
      "Epoch: 81 -> Loss: 0.004870779594785574\n",
      "Epoch: 82 -> Loss: 0.004868853867362166\n",
      "Epoch: 83 -> Loss: 0.004866940751006311\n",
      "Epoch: 84 -> Loss: 0.0048650380314300135\n",
      "Epoch: 85 -> Loss: 0.004863143997899903\n",
      "Epoch: 86 -> Loss: 0.00486125732838167\n",
      "Epoch: 87 -> Loss: 0.0048593770008833245\n",
      "Epoch: 88 -> Loss: 0.004857502225019929\n",
      "Epoch: 89 -> Loss: 0.004855632389187225\n",
      "Epoch: 90 -> Loss: 0.004853767019782969\n",
      "Epoch: 91 -> Loss: 0.00485190574972845\n",
      "Epoch: 92 -> Loss: 0.004850048294168956\n",
      "Epoch: 93 -> Loss: 0.004848194431716031\n",
      "Epoch: 94 -> Loss: 0.004846343989967583\n",
      "Epoch: 95 -> Loss: 0.004844496834330518\n",
      "Epoch: 96 -> Loss: 0.004842652859392756\n",
      "Epoch: 97 -> Loss: 0.004840811982263217\n",
      "Epoch: 98 -> Loss: 0.004838974137431408\n",
      "Epoch: 99 -> Loss: 0.004837139272799447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Value(data=0.004835307346619926"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 0.01\n",
    "for i in range(epochs):\n",
    "    pred = [mlp(xs[i]) for i in range(3)]\n",
    "    loss = mse(pred, target)\n",
    "    for p in mlp.parameters():\n",
    "        p.grad = .0\n",
    "\n",
    "    loss.backward()\n",
    "    for p in mlp.parameters():\n",
    "        p.value -= lr * p.grad\n",
    "    loss = mse(pred, target)\n",
    "\n",
    "    # lr\n",
    "    # if i % 100 == 0:\n",
    "    #     lr *= 0.9 # decay the learning rate\n",
    "    # this is a simple problem no need to decay lr\n",
    "\n",
    "    print(f\"Epoch: {i} -> Loss: {loss.value}\")\n",
    "\n",
    "pred = [mlp(xs[i]) for i in range(3)]\n",
    "loss = mse(pred, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wooooooow! It's working! Loss decreased by a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9511849156293668,\n",
       " Value(data=0.008335611698960703,\n",
       " Value(data=0.9511849156293668]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not perfect, but it's a good start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget zero grad after each epoch, because the gradients are accumulated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the results with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition tests pass\n",
      "Multiplication tests pass\n",
      "Tanh tests pass\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def test_addition():\n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    y = torch.tensor(3.0, requires_grad=True)\n",
    "    z = x + y\n",
    "    z.backward()\n",
    "    torch_grad_x = x.grad.item()\n",
    "    torch_grad_y = y.grad.item()\n",
    "\n",
    "    a = Value(2.0)\n",
    "    b = Value(3.0)\n",
    "    c = a + b\n",
    "    c.backward()\n",
    "    autodiff_grad_a = a.grad\n",
    "    autodiff_grad_b = b.grad\n",
    "\n",
    "    assert np.isclose(torch_grad_x, autodiff_grad_a), f\"Gradients do not match for x: {torch_grad_x} vs {autodiff_grad_a}\"\n",
    "    assert np.isclose(torch_grad_y, autodiff_grad_b), f\"Gradients do not match for y: {torch_grad_y} vs {autodiff_grad_b}\"\n",
    "\n",
    "    print(\"Addition tests pass\")\n",
    "\n",
    "def test_multiplication():\n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    y = torch.tensor(3.0, requires_grad=True)\n",
    "    z = x * y\n",
    "    z.backward()\n",
    "    torch_grad_x = x.grad.item()\n",
    "    torch_grad_y = y.grad.item()\n",
    "\n",
    "    a = Value(2.0)\n",
    "    b = Value(3.0)\n",
    "    c = a * b\n",
    "    c.backward()\n",
    "    autodiff_grad_a = a.grad\n",
    "    autodiff_grad_b = b.grad\n",
    "\n",
    "    assert np.isclose(torch_grad_x, autodiff_grad_a), f\"Gradients do not match for x: {torch_grad_x} vs {autodiff_grad_a}\"\n",
    "    assert np.isclose(torch_grad_y, autodiff_grad_b), f\"Gradients do not match for y: {torch_grad_y} vs {autodiff_grad_b}\"\n",
    "\n",
    "    print(\"Multiplication tests pass\")\n",
    "\n",
    "def test_tanh():\n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    y = torch.tanh(x)\n",
    "    y.backward()\n",
    "    torch_grad_x = x.grad.item()\n",
    "\n",
    "    a = Value(2.0)\n",
    "    b = a.tanh()\n",
    "    b.backward()\n",
    "    autodiff_grad_a = a.grad\n",
    "\n",
    "    assert np.isclose(torch_grad_x, autodiff_grad_a), f\"Gradients do not match for x: {torch_grad_x} vs {autodiff_grad_a}\"\n",
    "\n",
    "    print(\"Tanh tests pass\")\n",
    "\n",
    "test_addition()\n",
    "test_multiplication()\n",
    "test_tanh()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
