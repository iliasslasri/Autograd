{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=32 Value(data=2\n",
      "Value(data=34\n",
      "Value(data=64\n",
      "Value(data=96\n",
      "1.0 3.0 32.0\n"
     ]
    }
   ],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='') -> None:\n",
    "        self.value: int = data\n",
    "        self.grad = .0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value(data={self.value}\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # to make a+1.0 work\n",
    "        out = Value(self.value + other.value, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.value * other.value, (self, other), \"*\")\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return other * self\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.value\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), \"tanh\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = (1 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.value\n",
    "        out = Value(math.exp(x), (self,))\n",
    "        def _backward():\n",
    "            self.grad += out.value * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * other**(-1)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supports int/float powers\"\n",
    "        out = Value(self.value**other, (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * self.value**(other-1) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def backward(self):\n",
    "        # build the topological graph to order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "a = Value(32)\n",
    "b = Value(2)\n",
    "print(a, b)\n",
    "print(a+b)\n",
    "print(a*b)\n",
    "d = a*b + a\n",
    "print(d)\n",
    "\n",
    "d.backward()\n",
    "\n",
    "print(d.grad, a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Value(data=32, Value(data=64} +\n"
     ]
    }
   ],
   "source": [
    "print(d._prev, d._op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 5.0\n"
     ]
    }
   ],
   "source": [
    "# Bug, should be 2 instead of 1, in the function _backward of addition,\n",
    "#  we are overriding the gradient value (1), when self is the same object as other\n",
    "#  when we use a variable more then ones\n",
    "#  we should accumulate those gradients, multivariate case of chain rule\n",
    "s = a + a\n",
    "s.backward()\n",
    "print(s.grad, a.grad)\n",
    "# Expected 2, got 1 for s.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.0\n"
     ]
    }
   ],
   "source": [
    "# Bug fixed\n",
    "a = Value(32)\n",
    "s = a + a\n",
    "s.backward()\n",
    "print(s.grad, a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run back propagation, we need to store all the nodes in a topological order, then run the function in reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Value(data=-2.0"
      ]
     },
     "execution_count": 1222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(4.0)\n",
    "print(a/b)\n",
    "a-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From scalars, gradients, neurons, and backpropagation To a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9990327218788304"
      ]
     },
     "execution_count": 1223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, n) -> None:\n",
    "        self.w = [Value(np.random.randn()) for _ in range(n)]\n",
    "        self.b = Value(np.random.randn())\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        a = sum((wi+xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        return a.tanh()\n",
    "    \n",
    "    \n",
    "x = [2.0, 3.0]\n",
    "n = Neuron(2)\n",
    "n(x) # python will call n.__call__(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.999596205659595,\n",
       " Value(data=0.9998433421296711,\n",
       " Value(data=0.991058280631276]"
      ]
     },
     "execution_count": 1224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer:\n",
    "    def __init__(self, nin, nout) -> None:\n",
    "        self.nin = nin\n",
    "        self.nout = nout\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for neuron in self.neurons:\n",
    "            params.extend(neuron.parameters())\n",
    "        return params\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return [neuron(x) for neuron in self.neurons]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)[0] if self.nout == 1 else self.forward(x)\n",
    "    \n",
    "l = Layer(2, 3)\n",
    "l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, nin, nouts) -> None:\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.parameters())\n",
    "        return params\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.33283583596315486"
      ]
     },
     "execution_count": 1226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = [2.0, 3.0]\n",
    "mlp = MLP(2, [3, 4, 1]) \n",
    "mlp(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.33283583596315486,\n",
       " Value(data=0.3283814997418692,\n",
       " Value(data=0.33283583596315486]"
      ]
     },
     "execution_count": 1227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "xs = [\n",
    "    [2.0, 3.0, 1.0, 15.0],\n",
    "    [1.0, 2.0, 3.0, 4.0],\n",
    "    [3.0, 2.0, 1.0, 0.0]\n",
    "]\n",
    "target = [1.0, 0.0, 1.0]\n",
    "pred = [mlp(xs[i]) for i in range(3)]\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's define a loss to mesure the error of the model, we will use the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9980504529226839"
      ]
     },
     "execution_count": 1228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse(pred, target):\n",
    "    return sum((p-t)**2 for p, t in zip(pred, target))\n",
    "\n",
    "loss = mse(pred, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1229,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006039528886460024"
      ]
     },
     "execution_count": 1230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the gradients\n",
    "mlp.layers[0].neurons[1].w[1].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.0631581074830363,\n",
       " Value(data=1.2126200159077103,\n",
       " Value(data=0.5648248302090649,\n",
       " Value(data=0.07480969899065093,\n",
       " Value(data=-1.9751401083716467,\n",
       " Value(data=0.048336867329492744,\n",
       " Value(data=-0.12159686498545282,\n",
       " Value(data=0.4334831358751545,\n",
       " Value(data=1.0866112012114235,\n",
       " Value(data=0.7194646795587307,\n",
       " Value(data=0.8147173504410563,\n",
       " Value(data=-1.1336829860843465,\n",
       " Value(data=0.025497882664008813,\n",
       " Value(data=0.031398784797302876,\n",
       " Value(data=-0.7681163870856056,\n",
       " Value(data=-0.4916257897700847,\n",
       " Value(data=0.9443188708234825,\n",
       " Value(data=-0.4283991953581445,\n",
       " Value(data=-1.0158648100832557,\n",
       " Value(data=2.788910898271474,\n",
       " Value(data=0.5563309101972289,\n",
       " Value(data=1.0277910938614678,\n",
       " Value(data=-1.1566885349403597,\n",
       " Value(data=0.15453851050353687,\n",
       " Value(data=1.0241695195532723,\n",
       " Value(data=-1.7355261560225108,\n",
       " Value(data=0.8893762589785813,\n",
       " Value(data=-1.3011355519653087,\n",
       " Value(data=-0.31824260876749255,\n",
       " Value(data=-1.1768315220155177]"
      ]
     },
     "execution_count": 1231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights and biases of all the network\n",
    "mlp.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9751401083716467"
      ]
     },
     "execution_count": 1232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.layers[0].neurons[1].w[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9752005036605114"
      ]
     },
     "execution_count": 1233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.01\n",
    "for p in mlp.parameters():\n",
    "    p.value -= lr * p.grad\n",
    "\n",
    "# check the gradients\n",
    "mlp.layers[0].neurons[1].w[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.8611903741909768"
      ]
     },
     "execution_count": 1234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, we expect the loss to decrease\n",
    "pred = [mlp(xs[i]) for i in range(3)]\n",
    "loss = mse(pred, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreased. Yaaay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 -> Loss: 0.000750808907551976\n",
      "Epoch: 1 -> Loss: 0.000750693739365945\n",
      "Epoch: 2 -> Loss: 0.0007505786154831024\n",
      "Epoch: 3 -> Loss: 0.0007504635358783628\n",
      "Epoch: 4 -> Loss: 0.0007503485005266475\n",
      "Epoch: 5 -> Loss: 0.0007502335094029262\n",
      "Epoch: 6 -> Loss: 0.0007501185624821552\n",
      "Epoch: 7 -> Loss: 0.0007500036597393339\n",
      "Epoch: 8 -> Loss: 0.0007498888011494621\n",
      "Epoch: 9 -> Loss: 0.0007497739866875753\n",
      "Epoch: 10 -> Loss: 0.0007496592163287161\n",
      "Epoch: 11 -> Loss: 0.0007495444900479537\n",
      "Epoch: 12 -> Loss: 0.0007494298078203573\n",
      "Epoch: 13 -> Loss: 0.0007493151696210418\n",
      "Epoch: 14 -> Loss: 0.0007492005754251255\n",
      "Epoch: 15 -> Loss: 0.0007490860252077406\n",
      "Epoch: 16 -> Loss: 0.0007489715189440483\n",
      "Epoch: 17 -> Loss: 0.0007488570566092288\n",
      "Epoch: 18 -> Loss: 0.0007487426381784638\n",
      "Epoch: 19 -> Loss: 0.0007486282636269817\n",
      "Epoch: 20 -> Loss: 0.0007485139329299998\n",
      "Epoch: 21 -> Loss: 0.0007483996460627708\n",
      "Epoch: 22 -> Loss: 0.0007482854030005731\n",
      "Epoch: 23 -> Loss: 0.0007481712037186795\n",
      "Epoch: 24 -> Loss: 0.0007480570481923954\n",
      "Epoch: 25 -> Loss: 0.0007479429363970601\n",
      "Epoch: 26 -> Loss: 0.0007478288683080007\n",
      "Epoch: 27 -> Loss: 0.0007477148439005722\n",
      "Epoch: 28 -> Loss: 0.0007476008631501687\n",
      "Epoch: 29 -> Loss: 0.0007474869260321698\n",
      "Epoch: 30 -> Loss: 0.0007473730325220035\n",
      "Epoch: 31 -> Loss: 0.0007472591825951033\n",
      "Epoch: 32 -> Loss: 0.000747145376226904\n",
      "Epoch: 33 -> Loss: 0.0007470316133928899\n",
      "Epoch: 34 -> Loss: 0.0007469178940685561\n",
      "Epoch: 35 -> Loss: 0.0007468042182293838\n",
      "Epoch: 36 -> Loss: 0.0007466905858509143\n",
      "Epoch: 37 -> Loss: 0.0007465769969086883\n",
      "Epoch: 38 -> Loss: 0.0007464634513782567\n",
      "Epoch: 39 -> Loss: 0.0007463499492352138\n",
      "Epoch: 40 -> Loss: 0.0007462364904551435\n",
      "Epoch: 41 -> Loss: 0.0007461230750136646\n",
      "Epoch: 42 -> Loss: 0.0007460097028864102\n",
      "Epoch: 43 -> Loss: 0.0007458963740490246\n",
      "Epoch: 44 -> Loss: 0.0007457830884771866\n",
      "Epoch: 45 -> Loss: 0.0007456698461465828\n",
      "Epoch: 46 -> Loss: 0.0007455566470329142\n",
      "Epoch: 47 -> Loss: 0.0007454434911119019\n",
      "Epoch: 48 -> Loss: 0.0007453303783592864\n",
      "Epoch: 49 -> Loss: 0.0007452173087508388\n",
      "Epoch: 50 -> Loss: 0.0007451042822623197\n",
      "Epoch: 51 -> Loss: 0.0007449912988695335\n",
      "Epoch: 52 -> Loss: 0.0007448783585482857\n",
      "Epoch: 53 -> Loss: 0.0007447654612744148\n",
      "Epoch: 54 -> Loss: 0.0007446526070237613\n",
      "Epoch: 55 -> Loss: 0.0007445397957722049\n",
      "Epoch: 56 -> Loss: 0.0007444270274956095\n",
      "Epoch: 57 -> Loss: 0.0007443143021699033\n",
      "Epoch: 58 -> Loss: 0.0007442016197709801\n",
      "Epoch: 59 -> Loss: 0.0007440889802747913\n",
      "Epoch: 60 -> Loss: 0.0007439763836573005\n",
      "Epoch: 61 -> Loss: 0.0007438638298944563\n",
      "Epoch: 62 -> Loss: 0.0007437513189622765\n",
      "Epoch: 63 -> Loss: 0.0007436388508367529\n",
      "Epoch: 64 -> Loss: 0.0007435264254939135\n",
      "Epoch: 65 -> Loss: 0.0007434140429098025\n",
      "Epoch: 66 -> Loss: 0.0007433017030604929\n",
      "Epoch: 67 -> Loss: 0.000743189405922052\n",
      "Epoch: 68 -> Loss: 0.0007430771514705868\n",
      "Epoch: 69 -> Loss: 0.0007429649396821996\n",
      "Epoch: 70 -> Loss: 0.0007428527705330356\n",
      "Epoch: 71 -> Loss: 0.0007427406439992386\n",
      "Epoch: 72 -> Loss: 0.0007426285600569745\n",
      "Epoch: 73 -> Loss: 0.0007425165186824268\n",
      "Epoch: 74 -> Loss: 0.000742404519851815\n",
      "Epoch: 75 -> Loss: 0.0007422925635413333\n",
      "Epoch: 76 -> Loss: 0.0007421806497272412\n",
      "Epoch: 77 -> Loss: 0.0007420687783857835\n",
      "Epoch: 78 -> Loss: 0.0007419569494932408\n",
      "Epoch: 79 -> Loss: 0.0007418451630258891\n",
      "Epoch: 80 -> Loss: 0.0007417334189600591\n",
      "Epoch: 81 -> Loss: 0.0007416217172720507\n",
      "Epoch: 82 -> Loss: 0.0007415100579382266\n",
      "Epoch: 83 -> Loss: 0.0007413984409349308\n",
      "Epoch: 84 -> Loss: 0.0007412868662385523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2262.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85 -> Loss: 0.0007411753338254846\n",
      "Epoch: 86 -> Loss: 0.0007410638436721379\n",
      "Epoch: 87 -> Loss: 0.0007409523957549446\n",
      "Epoch: 88 -> Loss: 0.0007408409900503457\n",
      "Epoch: 89 -> Loss: 0.0007407296265348111\n",
      "Epoch: 90 -> Loss: 0.0007406183051848161\n",
      "Epoch: 91 -> Loss: 0.000740507025976869\n",
      "Epoch: 92 -> Loss: 0.0007403957888874795\n",
      "Epoch: 93 -> Loss: 0.0007402845938931835\n",
      "Epoch: 94 -> Loss: 0.0007401734409705333\n",
      "Epoch: 95 -> Loss: 0.000740062330096094\n",
      "Epoch: 96 -> Loss: 0.0007399512612464443\n",
      "Epoch: 97 -> Loss: 0.0007398402343981988\n",
      "Epoch: 98 -> Loss: 0.0007397292495279734\n",
      "Epoch: 99 -> Loss: 0.0007396183066124045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Value(data=0.0007395074056281415"
      ]
     },
     "execution_count": 1311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 0.01\n",
    "for i in tqdm(range(epochs)):\n",
    "    pred = [mlp(xs[i]) for i in range(3)]\n",
    "    loss = mse(pred, target)\n",
    "    for p in mlp.parameters():\n",
    "        p.grad = .0\n",
    "\n",
    "    loss.backward()\n",
    "    for p in mlp.parameters():\n",
    "        p.value -= lr * p.grad\n",
    "    loss = mse(pred, target)\n",
    "\n",
    "    # lr\n",
    "    # if i % 100 == 0:\n",
    "    #     lr *= 0.9 # decay the learning rate\n",
    "    # this is a simple problem no need to decay lr\n",
    "\n",
    "    print(f\"Epoch: {i} -> Loss: {loss.value}\")\n",
    "\n",
    "pred = [mlp(xs[i]) for i in range(3)]\n",
    "loss = mse(pred, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wooooooow! It's working! Loss decreased by a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9807835367697398,\n",
       " Value(data=0.0009810644567304848,\n",
       " Value(data=0.9807835367697398]"
      ]
     },
     "execution_count": 1312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not perfect, but it's a good start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget zero grad after each epoch, because the gradients are accumulated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
