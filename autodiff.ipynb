{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=32, size=56Bytes) Value(data=2, size=56Bytes)\n",
      "Value(data=34, size=56Bytes)\n",
      "Value(data=64, size=56Bytes)\n",
      "Value(data=96, size=56Bytes)\n",
      "1.0 3.0 32.0\n"
     ]
    }
   ],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='') -> None:\n",
    "        self.value: int = data\n",
    "        self.grad = .0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value(data={self.value}, size={sys.getsizeof(self)}Bytes)\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # to make a+1.0 work\n",
    "        out = Value(self.value + other.value, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.value * other.value, (self, other), \"*\")\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return other * self\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.value\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), \"tanh\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = (1 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.value\n",
    "        out = Value(math.exp(x), (self,))\n",
    "        def _backward():\n",
    "            self.grad += out.value * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * other**(-1)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supports int/float powers\"\n",
    "        out = Value(self.value**other, (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * self.value**(other-1) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def backward(self):\n",
    "        # build the topological graph to order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "a = Value(32)\n",
    "b = Value(2)\n",
    "print(a, b)\n",
    "print(a+b)\n",
    "print(a*b)\n",
    "d = a*b + a\n",
    "print(d)\n",
    "\n",
    "d.backward()\n",
    "\n",
    "print(d.grad, a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Value(data=32, size=56Bytes), Value(data=64, size=56Bytes)} +\n"
     ]
    }
   ],
   "source": [
    "print(d._prev, d._op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 5.0\n"
     ]
    }
   ],
   "source": [
    "# Bug, should be 2 instead of 1, in the function _backward of addition,\n",
    "#  we are overriding the gradient value (1), when self is the same object as other\n",
    "#  when we use a variable more then ones\n",
    "#  we should accumulate those gradients, multivariate case of chain rule\n",
    "s = a + a\n",
    "s.backward()\n",
    "print(s.grad, a.grad)\n",
    "# Expected 2, got 1 for s.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.0\n"
     ]
    }
   ],
   "source": [
    "# Bug fixed\n",
    "a = Value(32)\n",
    "s = a + a\n",
    "s.backward()\n",
    "print(s.grad, a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run back propagation, we need to store all the nodes in a topological order, then run the function in reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.5, size=56Bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Value(data=-2.0, size=56Bytes)"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(4.0)\n",
    "print(a/b)\n",
    "a-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From scalars, gradients, neurons, and backpropagation To a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9897511152845961, size=56Bytes)"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, n) -> None:\n",
    "        self.w = [Value(np.random.randn()) for _ in range(n)]\n",
    "        self.b = Value(np.random.randn())\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        a = sum((wi+xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        return a.tanh()\n",
    "    \n",
    "    \n",
    "x = [2.0, 3.0]\n",
    "n = Neuron(2)\n",
    "n(x) # python will call n.__call__(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.999997338164193, size=56Bytes),\n",
       " Value(data=0.9999530116258976, size=56Bytes),\n",
       " Value(data=0.9998719950881702, size=56Bytes)]"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer:\n",
    "    def __init__(self, nin, nout) -> None:\n",
    "        self.nin = nin\n",
    "        self.nout = nout\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for neuron in self.neurons:\n",
    "            params.extend(neuron.parameters())\n",
    "        return params\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return [neuron(x) for neuron in self.neurons]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)[0] if self.nout == 1 else self.forward(x)\n",
    "    \n",
    "l = Layer(2, 3)\n",
    "l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, nin, nouts) -> None:\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.parameters())\n",
    "        return params\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9145138093877023, size=56Bytes)"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = [2.0, 3.0]\n",
    "mlp = MLP(2, [3, 4, 1]) \n",
    "mlp(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9145138093877023, size=56Bytes),\n",
       " Value(data=0.8566918735805565, size=56Bytes),\n",
       " Value(data=0.9145138093877023, size=56Bytes)]"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "xs = [\n",
    "    [2.0, 3.0, 1.0, 15.0],\n",
    "    [1.0, 2.0, 3.0, 4.0],\n",
    "    [3.0, 2.0, 1.0, 0.0]\n",
    "]\n",
    "target = [1.0, 0.0, 1.0]\n",
    "pred = [mlp(xs[i]) for i in range(3)]\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's define a loss to mesure the error of the model, we will use the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.7485367438297685, size=56Bytes)"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse(pred, target):\n",
    "    return sum((p-t)**2 for p, t in zip(pred, target))\n",
    "\n",
    "loss = mse(pred, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00025144986133308965"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the gradients\n",
    "mlp.layers[0].neurons[1].w[1].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.5794855995877302, size=56Bytes),\n",
       " Value(data=0.33667057157449, size=56Bytes),\n",
       " Value(data=-1.7907542989818883, size=56Bytes),\n",
       " Value(data=1.4763437050314083, size=56Bytes),\n",
       " Value(data=-0.7884670309878907, size=56Bytes),\n",
       " Value(data=0.7861819938845216, size=56Bytes),\n",
       " Value(data=-1.0336710139717482, size=56Bytes),\n",
       " Value(data=1.4241566140922832, size=56Bytes),\n",
       " Value(data=-0.23333981591318337, size=56Bytes),\n",
       " Value(data=0.8016614973491538, size=56Bytes),\n",
       " Value(data=0.49305397053304234, size=56Bytes),\n",
       " Value(data=1.5241419952773227, size=56Bytes),\n",
       " Value(data=-0.30559916173037777, size=56Bytes),\n",
       " Value(data=0.06643229635308645, size=56Bytes),\n",
       " Value(data=-1.3821969756051278, size=56Bytes),\n",
       " Value(data=-0.9402984078802232, size=56Bytes),\n",
       " Value(data=-2.477302392789485, size=56Bytes),\n",
       " Value(data=0.032850974260755256, size=56Bytes),\n",
       " Value(data=-0.24643604232438562, size=56Bytes),\n",
       " Value(data=1.6388662445323356, size=56Bytes),\n",
       " Value(data=-0.41827466568077204, size=56Bytes),\n",
       " Value(data=0.8835935960018325, size=56Bytes),\n",
       " Value(data=-1.0203757375294502, size=56Bytes),\n",
       " Value(data=-1.713883421740003, size=56Bytes),\n",
       " Value(data=-0.9967023785491071, size=56Bytes),\n",
       " Value(data=0.43199184417079994, size=56Bytes),\n",
       " Value(data=-0.628099413040545, size=56Bytes),\n",
       " Value(data=1.4640697479930513, size=56Bytes),\n",
       " Value(data=1.7782878349051006, size=56Bytes),\n",
       " Value(data=-2.6972556909728413, size=56Bytes)]"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights and biases of all the network\n",
    "mlp.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7884670309878907"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.layers[0].neurons[1].w[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.788469545486504"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.01\n",
    "for p in mlp.parameters():\n",
    "    p.value -= lr * p.grad\n",
    "\n",
    "# check the gradients\n",
    "mlp.layers[0].neurons[1].w[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.7325330459306472, size=56Bytes)"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, we expect the loss to decrease\n",
    "pred = [mlp(xs[i]) for i in range(3)]\n",
    "loss = mse(pred, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreased. Yaaay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2342.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 -> Loss: 0.7325330459306472\n",
      "Epoch: 1 -> Loss: 0.6979110704977577\n",
      "Epoch: 2 -> Loss: 0.6400649335021132\n",
      "Epoch: 3 -> Loss: 0.5566683828835824\n",
      "Epoch: 4 -> Loss: 0.4646470756745054\n",
      "Epoch: 5 -> Loss: 0.43291084595840634\n",
      "Epoch: 6 -> Loss: 0.5428250453286493\n",
      "Epoch: 7 -> Loss: 0.6797703315599455\n",
      "Epoch: 8 -> Loss: 0.6129126201656261\n",
      "Epoch: 9 -> Loss: 0.40776361660678984\n",
      "Epoch: 10 -> Loss: 0.29817863502867387\n",
      "Epoch: 11 -> Loss: 0.3130730187356399\n",
      "Epoch: 12 -> Loss: 0.36119040189273593\n",
      "Epoch: 13 -> Loss: 0.38643189811000983\n",
      "Epoch: 14 -> Loss: 0.3723274489097933\n",
      "Epoch: 15 -> Loss: 0.31910053907413105\n",
      "Epoch: 16 -> Loss: 0.23654395678629767\n",
      "Epoch: 17 -> Loss: 0.14460882493119648\n",
      "Epoch: 18 -> Loss: 0.07216042802003171\n",
      "Epoch: 19 -> Loss: 0.04864357789468318\n",
      "Epoch: 20 -> Loss: 0.08769353975357973\n",
      "Epoch: 21 -> Loss: 0.17296625063200427\n",
      "Epoch: 22 -> Loss: 0.25497680113704724\n",
      "Epoch: 23 -> Loss: 0.26884124846816077\n",
      "Epoch: 24 -> Loss: 0.19667809519030777\n",
      "Epoch: 25 -> Loss: 0.1093393329231052\n",
      "Epoch: 26 -> Loss: 0.09420190444657436\n",
      "Epoch: 27 -> Loss: 0.16743083004181983\n",
      "Epoch: 28 -> Loss: 0.2826043998415365\n",
      "Epoch: 29 -> Loss: 0.3903758638646572\n",
      "Epoch: 30 -> Loss: 0.4672312211743993\n",
      "Epoch: 31 -> Loss: 0.5084400899265232\n",
      "Epoch: 32 -> Loss: 0.5154142362242654\n",
      "Epoch: 33 -> Loss: 0.4899854852988957\n",
      "Epoch: 34 -> Loss: 0.43694246734707165\n",
      "Epoch: 35 -> Loss: 0.3794298119838937\n",
      "Epoch: 36 -> Loss: 0.38584327964807846\n",
      "Epoch: 37 -> Loss: 0.5225939904620667\n",
      "Epoch: 38 -> Loss: 0.6510414273506466\n",
      "Epoch: 39 -> Loss: 0.5734332828446114\n",
      "Epoch: 40 -> Loss: 0.44176388587832305\n",
      "Epoch: 41 -> Loss: 0.43257778775777184\n",
      "Epoch: 42 -> Loss: 0.4945201993647861\n",
      "Epoch: 43 -> Loss: 0.5566529565439792\n",
      "Epoch: 44 -> Loss: 0.5969901871963139\n",
      "Epoch: 45 -> Loss: 0.6141564303586505\n",
      "Epoch: 46 -> Loss: 0.6102496045473323\n",
      "Epoch: 47 -> Loss: 0.5864600183445555\n",
      "Epoch: 48 -> Loss: 0.5433299460568213\n",
      "Epoch: 49 -> Loss: 0.4838380977950161\n",
      "Epoch: 50 -> Loss: 0.42051093029993647\n",
      "Epoch: 51 -> Loss: 0.38385024845183685\n",
      "Epoch: 52 -> Loss: 0.4087524793576153\n",
      "Epoch: 53 -> Loss: 0.4724776472425871\n",
      "Epoch: 54 -> Loss: 0.47402274446720505\n",
      "Epoch: 55 -> Loss: 0.36022947316784426\n",
      "Epoch: 56 -> Loss: 0.2090316662257478\n",
      "Epoch: 57 -> Loss: 0.12084927568978521\n",
      "Epoch: 58 -> Loss: 0.10958408225919686\n",
      "Epoch: 59 -> Loss: 0.13405261622957232\n",
      "Epoch: 60 -> Loss: 0.15928695856452782\n",
      "Epoch: 61 -> Loss: 0.16947784117986936\n",
      "Epoch: 62 -> Loss: 0.15943292968484143\n",
      "Epoch: 63 -> Loss: 0.12826774655727738\n",
      "Epoch: 64 -> Loss: 0.0793504978307309\n",
      "Epoch: 65 -> Loss: 0.02724901503765545\n",
      "Epoch: 66 -> Loss: 0.0022742365312498395\n",
      "Epoch: 67 -> Loss: 0.02010537945815816\n",
      "Epoch: 68 -> Loss: 0.04229876887866089\n",
      "Epoch: 69 -> Loss: 0.02683744267630169\n",
      "Epoch: 70 -> Loss: 0.0105258390751664\n",
      "Epoch: 71 -> Loss: 0.0443169481577923\n",
      "Epoch: 72 -> Loss: 0.07199705951019267\n",
      "Epoch: 73 -> Loss: 0.06819418798906561\n",
      "Epoch: 74 -> Loss: 0.09507788513483698\n",
      "Epoch: 75 -> Loss: 0.20642066897230815\n",
      "Epoch: 76 -> Loss: 0.3420922997529376\n",
      "Epoch: 77 -> Loss: 0.37827033729454423\n",
      "Epoch: 78 -> Loss: 0.31368588491143823\n",
      "Epoch: 79 -> Loss: 0.28964095943209156\n",
      "Epoch: 80 -> Loss: 0.34398193440325064\n",
      "Epoch: 81 -> Loss: 0.41337075148459107\n",
      "Epoch: 82 -> Loss: 0.4571384206178472\n",
      "Epoch: 83 -> Loss: 0.4645178677243962\n",
      "Epoch: 84 -> Loss: 0.4352012232571836\n",
      "Epoch: 85 -> Loss: 0.3790915175007491\n",
      "Epoch: 86 -> Loss: 0.33757452882601297\n",
      "Epoch: 87 -> Loss: 0.38011691430235195\n",
      "Epoch: 88 -> Loss: 0.4531682876662697\n",
      "Epoch: 89 -> Loss: 0.3801989667483632\n",
      "Epoch: 90 -> Loss: 0.23725738523563789\n",
      "Epoch: 91 -> Loss: 0.1915470565732118\n",
      "Epoch: 92 -> Loss: 0.20293706860571314\n",
      "Epoch: 93 -> Loss: 0.1821720213895677\n",
      "Epoch: 94 -> Loss: 0.10722514176489653\n",
      "Epoch: 95 -> Loss: 0.035770709860620566\n",
      "Epoch: 96 -> Loss: 0.027995417432868073\n",
      "Epoch: 97 -> Loss: 0.04641205519595132\n",
      "Epoch: 98 -> Loss: 0.0457293076388307\n",
      "Epoch: 99 -> Loss: 0.02678905942250209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Value(data=0.012765924946486328, size=56Bytes)"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 0.01\n",
    "for i in tqdm(range(epochs)):\n",
    "    pred = [mlp(xs[i]) for i in range(3)]\n",
    "    loss = mse(pred, target)\n",
    "    loss.backward()\n",
    "    for p in mlp.parameters():\n",
    "        p.value -= lr * p.grad\n",
    "    loss = mse(pred, target)\n",
    "    print(f\"Epoch: {i} -> Loss: {loss.value}\")\n",
    "\n",
    "pred = [mlp(xs[i]) for i in range(3)]\n",
    "loss = mse(pred, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wooooooow! It's working! Loss decreased by a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9201407818053836, size=56Bytes),\n",
       " Value(data=0.0033068845119885054, size=56Bytes),\n",
       " Value(data=0.9201407818053836, size=56Bytes)]"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not perfect, but it's a good start."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
