# Autograd

This is a simple non optimized implementation of auto diffrentiation with back-prop (reverse autodiff). It is not optimized for speed or memory usage. This is a simple implementation to understand the concept of autodiff and nn training.

Forwards and backwards pass are implemented for the following operations:
- Addition
- Subtraction
- Multiplication
- Division
- Power
- (more to come)

For the present time only scalar operations are supported.